{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samuel Shiao\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from dict import contractions, stop_words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(1234)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 5000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Data\n",
    "\n",
    "#### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'train.csv'\n",
    "#TEST_CSV = 'test.csv'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz' #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "#test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "#need to split data still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Evening shadows make me blue When each weary d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Ohhhhh ohhhhhh ohhhhhh ohhhhhhh  Oh her eyes h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy</td>\n",
       "      <td>[Ja Rule]  Woo ha ha right back at ya..  its t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Good times These are the good times Leave your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy</td>\n",
       "      <td>You were a child Crawling on your knees toward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Happy</td>\n",
       "      <td>I never thought that you would be the one to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Beauty queen of only eighteen She had some tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Tell me we dont have to leave Say we can stay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Sunday morning rain is falling Steal some cove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Isnt it a little late Shouldnt you fly away Li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                             Lyrics\n",
       "0  Happy  Evening shadows make me blue When each weary d...\n",
       "1  Happy  Ohhhhh ohhhhhh ohhhhhh ohhhhhhh  Oh her eyes h...\n",
       "2  Happy  [Ja Rule]  Woo ha ha right back at ya..  its t...\n",
       "3  Happy  Good times These are the good times Leave your...\n",
       "4  Happy  You were a child Crawling on your knees toward...\n",
       "5  Happy  I never thought that you would be the one to h...\n",
       "6  Happy  Beauty queen of only eighteen She had some tro...\n",
       "7  Happy  Tell me we dont have to leave Say we can stay ...\n",
       "8  Happy  Sunday morning rain is falling Steal some cove...\n",
       "9  Happy  Isnt it a little late Shouldnt you fly away Li..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Processing\n",
    "\n",
    "From Practical 4 Code:\n",
    "In this section we will proccess our data into a format suitable for inputting into a LSTM or more specifically, a Siamese LSTM. The general structure we will follow is:\n",
    "\n",
    "1. Preprocess the text (clean it up)\n",
    "2. Tokenize every word in the text (replace each word with an index number)\n",
    "3. Pad every sequence of indices with zeros to make them all the same length\n",
    "4. Build an embedding matrix that we can use to look up every indices word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Please preprocess the text. This is a standard practice to make sure that the text isn't noisy. Some examples of what you can do here are:\n",
    "\n",
    "* Removing stop words\n",
    "* Removing punctuation\n",
    "* Getting rid of stuff like \"what's\" and making it \"what is'\n",
    "* Stemming words so they are all the same tense (e.g. ran -> run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "punctuation = [\"\\\"\", \"?\", \"!\", \".\", \",\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    #Replace stop words\n",
    "    for w in stop_words:\n",
    "        text = text.replace(\" \" + w + \" \", \" \")\n",
    "    \n",
    "    #expanding contractions\n",
    "    words = text.split()\n",
    "    text = \"\"\n",
    "    for w in words:\n",
    "        if(str(w) in contractions):\n",
    "            text = text + contractions[w] + \" \"\n",
    "        else:\n",
    "            text = text + w + \" \"\n",
    "    text = text.strip() #to remove that last space at the end\n",
    "    \n",
    "    #Replace punctuation\n",
    "    for w in punctuation:\n",
    "        text = text.replace(w, \"\")\n",
    "    \n",
    "    #change verb tenses\n",
    "    words = text.split()\n",
    "    text = \"\"\n",
    "    for w in words:\n",
    "        text = text + str(ps.stem(w)) + \" \"\n",
    "    \n",
    "    # Return type should be str\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a basic sentence including normal syntax, but it has a few stop words in it. having have had flew fly flying\n",
      "thi basic sentenc includ normal syntax stop word it flew fli fli \n",
      "\n",
      "\n",
      "This sentence has more than just a few basic stop words! as if it were he they no top?? run running ran runner buying bought buy sought seek saw\n",
      "thi sentenc basic stop word top run run ran runner buy bought buy sought seek saw \n",
      "\n",
      "\n",
      "running runner ran runs flying flyer fly flies flight you've i'd he'd we'd\n",
      "run runner ran run fli flyer fli fli flight i would he had we would \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing clean_text\n",
    "\n",
    "test1 = \"This is a basic sentence including normal syntax, but it has a few stop words in it. having have had flew fly flying\"\n",
    "test2 = \"This sentence has more than just a few basic stop words! as if it were he they no top?? run running ran runner buying bought buy sought seek saw\"\n",
    "test3 = \"running runner ran runs flying flyer fly flies flight you've i'd he'd we'd\"\n",
    "\n",
    "\n",
    "print(test1)\n",
    "print(clean_text(test1))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(test2)\n",
    "print(clean_text(test2))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(test3)\n",
    "print(clean_text(test3))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the preprocessing `clean_text` function to every element in the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Training Data\n"
     ]
    }
   ],
   "source": [
    "lyrics = [clean_text(x) for x in train_df['Lyrics']]\n",
    "labels = train_df['Genre']\n",
    "print('Loaded Training Data')\n",
    "\n",
    "#X_test_1 = [clean_text(x) for x in test_df['question1']]\n",
    "#X_test_2 = [clean_text(x) for x in test_df['question2']]\n",
    "#print('Loaded Testing Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "To avoid manually having to assign indices and filtering out unfrequent words, we can use a Tokenizer to do this for us. It essentially creates a map of every unique word and an assigned index to it. We specify a parameter called `num_words` which says to only care about the top 20000 most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Building Tokenizer\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "print('Finished Building Tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the tokenizer to the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Tokenizing Training\n"
     ]
    }
   ],
   "source": [
    "train_sequences_1 = tokenizer.texts_to_sequences(lyrics)\n",
    "print('Finished Tokenizing Training')\n",
    "\n",
    "#test_sequences_1 = tokenizer.texts_to_sequences(X_test_1)\n",
    "#test_sequences_2 = tokenizer.texts_to_sequences(X_test_2)\n",
    "#print('Finished Tokenizing Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique words in tokenizer. Has to be <= 20,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3532 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad sequences all to the same length of 30 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (226, 5000)\n",
      "Shape of label tensor: (226,)\n",
      "Finished Padding Training\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', train_data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Finished Padding Training')\n",
    "\n",
    "#test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#print('Finished Padding Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix\n",
    "\n",
    "The embedding matrix is a `n x m` matrix where `n` is the number of words and `m` is the dimension of the embedding. In our case, `m=300` and `n=20000`. We take the min between the number of unique words in our tokenizer and max words in case there are less unique words than the max we specified. \n",
    "\n",
    "Row `i` in the matrix should contain the embedding of the word with index `i` in the tokenizer. An easy way to create this would be to iterate over `word_index.items()` which gives you the word and it's index. Keep in mind that you can't generate an embedding for a word not in your word2vec model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:23: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "\n",
    "#questions_cols = ['question1', 'question2']\n",
    "\n",
    "for dataset in [train_df]:#, test_df]:\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        q2n = []  # q2n -> question numbers representation\n",
    "        for word in clean_text(row['Lyrics']):\n",
    "\n",
    "\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(inverse_vocabulary)\n",
    "                q2n.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(word)\n",
    "            else:\n",
    "                q2n.append(vocabulary[word])\n",
    "\n",
    "        # Replace questions as word to question as number representation\n",
    "        dataset.set_value(index, 'Lyrics', q2n)\n",
    "            \n",
    "embedding_matrix = 1 * np.random.randn(len(vocabulary) + 1, EMBEDDING_DIM)  # This will be the embedding matrix\n",
    "embedding_matrix[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[index] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data\n",
    "\n",
    "Here we just format the data into each half for the input (left and right). There is no code to write here but understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Creating Training Data\n"
     ]
    }
   ],
   "source": [
    "# Random shuffle\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train]))#, train_data_2[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "print('Finished Creating Training Data')\n",
    "\n",
    "#data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "#labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "#print('Finished Creating Validation Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Building the Model\n",
    "\n",
    "In this section you will write code to build the actual Siamese network. It should take in two arguments (question1 and question2) and then output a single number representing the probability that the two questions are duplicates.\n",
    "\n",
    "The model should take in each input sentence, replace it with it's embeddings, then run the new embedding vector through a LSTM layer. The output of each LSTM layer should be concatenated together and then a standard Dense model can be used.\n",
    "\n",
    "Make sure to note that you should only use one LSTM layer that is shared by both the left and the right half. \n",
    "\n",
    "Make sure to title your output layers as `predictions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 5000, 128)         256000    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 5000, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lyric_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 128,input_length = 5000))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "#predictions = model.predict(data_1_train.reshape(1,5000),batch_size=1,verbose = 2)[0]\n",
    "#model.add(LSTM(20000, return_sequences=True))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(10, activation='softmax'))\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "#embedding\n",
    "#LSTM\n",
    "#hidden1\n",
    "#hidden2\n",
    "#hidden3\n",
    "#dense\n",
    "#output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(inputs=lyric_input, outputs=predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-345aaa3295f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lyrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(train_1_data['Lyrics']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Training the Model\n",
    "\n",
    "In this section we will simply train the model. We use the Early Stopping argument to end training if the loss or accuracy don't improve within 3 epochs.\n",
    "\n",
    "Since the training time is incredibly long (30 minutes or so on a CPU), only train it for one epoch if you don't have time. For better results, train it to around 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "hist = model.fit([data_1_train], labels_train, \\\n",
    "        validation_data=([data_1_val], labels_val), \\\n",
    "        epochs=1, batch_size=2048, shuffle=True, \\\n",
    "        callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
