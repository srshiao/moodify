{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samuel Shiao\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from dict import contractions, stop_words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "np.random.seed(1234)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 5000\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Data\n",
    "\n",
    "#### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'train.csv'\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Evening shadows make me blue When each weary d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Ohhhhh ohhhhhh ohhhhhh ohhhhhhh  Oh her eyes h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy</td>\n",
       "      <td>[Ja Rule]  Woo ha ha right back at ya..  its t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Good times These are the good times Leave your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy</td>\n",
       "      <td>You were a child Crawling on your knees toward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Happy</td>\n",
       "      <td>I never thought that you would be the one to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Beauty queen of only eighteen She had some tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Tell me we dont have to leave Say we can stay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Sunday morning rain is falling Steal some cove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Happy</td>\n",
       "      <td>Isnt it a little late Shouldnt you fly away Li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                             Lyrics\n",
       "0  Happy  Evening shadows make me blue When each weary d...\n",
       "1  Happy  Ohhhhh ohhhhhh ohhhhhh ohhhhhhh  Oh her eyes h...\n",
       "2  Happy  [Ja Rule]  Woo ha ha right back at ya..  its t...\n",
       "3  Happy  Good times These are the good times Leave your...\n",
       "4  Happy  You were a child Crawling on your knees toward...\n",
       "5  Happy  I never thought that you would be the one to h...\n",
       "6  Happy  Beauty queen of only eighteen She had some tro...\n",
       "7  Happy  Tell me we dont have to leave Say we can stay ...\n",
       "8  Happy  Sunday morning rain is falling Steal some cove...\n",
       "9  Happy  Isnt it a little late Shouldnt you fly away Li..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Processing\n",
    "\n",
    "From Practical 4 Code:\n",
    "In this section we will proccess our data into a format suitable for inputting into a LSTM or more specifically, a Siamese LSTM. The general structure we will follow is:\n",
    "\n",
    "1. Preprocess the text (clean it up)\n",
    "2. Tokenize every word in the text (replace each word with an index number)\n",
    "3. Pad every sequence of indices with zeros to make them all the same length\n",
    "4. Build an embedding matrix that we can use to look up every indices word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Please preprocess the text. This is a standard practice to make sure that the text isn't noisy. Some examples of what you can do here are:\n",
    "\n",
    "* Removing stop words\n",
    "* Removing punctuation\n",
    "* Getting rid of stuff like \"what's\" and making it \"what is'\n",
    "* Stemming words so they are all the same tense (e.g. ran -> run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "punctuation = [\"\\\"\", \"?\", \"!\", \".\", \",\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    \n",
    "    #Replace stop words\n",
    "    #for w in stop_words:\n",
    "    #    text = text.replace(\" \" + w + \" \", \" \")\n",
    "        \n",
    "    words = [x for x in \"i am the cop\".split() if x not in stop_words]\n",
    "    #expanding contractions\n",
    "    words = text.split()\n",
    "    \n",
    "    text = \"\"\n",
    "    for w in words:\n",
    "        if(str(w) in contractions):\n",
    "            text = text + contractions[w] + \" \"\n",
    "        else:\n",
    "            text = text + w + \" \"\n",
    "    text = text.strip() #to remove that last space at the end\n",
    "    \n",
    "    #Replace punctuation\n",
    "    for w in punctuation:\n",
    "        text = text.replace(w, \"\")\n",
    "    \n",
    "    #change verb tenses\n",
    "    words = text.split()\n",
    "    text = \"\"\n",
    "    for w in words:\n",
    "        text = text + str(ps.stem(w)) + \" \"\n",
    "    \n",
    "    # Return type should be str\n",
    "    return text;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a basic sentence including normal syntax, but it has a few stop words in it. having have had flew fly flying\n",
      "thi is a basic sentenc includ normal syntax but it ha a few stop word in it have have had flew fli fli \n",
      "\n",
      "\n",
      "This sentence has more than just a few basic stop words! as if it were he they no top?? run running ran runner buying bought buy sought seek saw\n",
      "thi sentenc ha more than just a few basic stop word as if it were he they no top run run ran runner buy bought buy sought seek saw \n",
      "\n",
      "\n",
      "running runner ran runs flying flyer fly flies flight you've i'd he'd we'd\n",
      "run runner ran run fli flyer fli fli flight you have i would he had we would \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing clean_text\n",
    "\n",
    "test1 = \"This is a basic sentence including normal syntax, but it has a few stop words in it. having have had flew fly flying\"\n",
    "test2 = \"This sentence has more than just a few basic stop words! as if it were he they no top?? run running ran runner buying bought buy sought seek saw\"\n",
    "test3 = \"running runner ran runs flying flyer fly flies flight you've i'd he'd we'd\"\n",
    "\n",
    "\n",
    "print(test1)\n",
    "print(clean_text(test1))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(test2)\n",
    "print(clean_text(test2))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(test3)\n",
    "print(clean_text(test3))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the preprocessing `clean_text` function to every element in the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Training Data\n"
     ]
    }
   ],
   "source": [
    "lyrics = [clean_text(x) for x in train_df['Lyrics']]\n",
    "labels = train_df['Genre']\n",
    "print('Loaded Training Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "To avoid manually having to assign indices and filtering out unfrequent words, we can use a Tokenizer to do this for us. It essentially creates a map of every unique word and an assigned index to it. We specify a parameter called `num_words` which says to only care about the top 20000 most frequent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Building Tokenizer\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "print('Finished Building Tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the tokenizer to the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Tokenizing Training\n"
     ]
    }
   ],
   "source": [
    "train_sequences_1 = tokenizer.texts_to_sequences(lyrics)\n",
    "print('Finished Tokenizing Training')\n",
    "\n",
    "#test_sequences_1 = tokenizer.texts_to_sequences(X_test_1)\n",
    "#test_sequences_2 = tokenizer.texts_to_sequences(X_test_2)\n",
    "#print('Finished Tokenizing Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique words in tokenizer. Has to be <= 20,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3539 unique tokens\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad sequences all to the same length of 30 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (226, 5000)\n",
      "Shape of label tensor: (226,)\n",
      "Finished Padding Training\n"
     ]
    }
   ],
   "source": [
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#train_data_2 = pad_sequences(train_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', train_data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print('Finished Padding Training')\n",
    "\n",
    "#test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#print('Finished Padding Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix\n",
    "\n",
    "The embedding matrix is a `n x m` matrix where `n` is the number of words and `m` is the dimension of the embedding. In our case, `m=300` and `n=20000`. We take the min between the number of unique words in our tokenizer and max words in case there are less unique words than the max we specified. \n",
    "\n",
    "Row `i` in the matrix should contain the embedding of the word with index `i` in the tokenizer. An easy way to create this would be to iterate over `word_index.items()` which gives you the word and it's index. Keep in mind that you can't generate an embedding for a word not in your word2vec model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "\n",
    "#questions_cols = ['question1', 'question2']\n",
    "\n",
    "for dataset in [train_df]:#, test_df]:\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        q2n = []  # q2n -> question numbers representation\n",
    "        for word in clean_text(row['Lyrics']):\n",
    "\n",
    "\n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = len(inverse_vocabulary)\n",
    "                q2n.append(len(inverse_vocabulary))\n",
    "                inverse_vocabulary.append(word)\n",
    "            else:\n",
    "                q2n.append(vocabulary[word])\n",
    "\n",
    "        # Replace questions as word to question as number representation\n",
    "        dataset.set_value(index, 'Lyrics', q2n)\n",
    "            \n",
    "embedding_matrix = 1 * np.random.randn(len(vocabulary) + 1, EMBEDDING_DIM)  # This will be the embedding matrix\n",
    "embedding_matrix[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "# Build the embedding matrix\n",
    "for word, index in vocabulary.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[index] = word2vec.word_vec(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data\n",
    "\n",
    "Here we just format the data into each half for the input (left and right). There is no code to write here but understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Creating Training Data\n"
     ]
    }
   ],
   "source": [
    "# Random shuffle\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = train_data_1[idx_train]#, train_data_2[idx_train]))\n",
    "labels_train = []\n",
    "for label in labels[idx_train]:\n",
    "    if(label == 'Happy'):\n",
    "        labels_train.append([0, 1, 0])\n",
    "    elif(label == 'Sad'):\n",
    "        labels_train.append([1, 0, 0])\n",
    "    else:\n",
    "        labels_train.append([0, 0, 1])\n",
    "    \n",
    "labels_train = np.array(labels_train)\n",
    "print('Finished Creating Training Data')\n",
    "\n",
    "#data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "#labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "#print('Finished Creating Validation Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Building the Model\n",
    "\n",
    "In this section you will write code to build the actual Siamese network. It should take in two arguments (question1 and question2) and then output a single number representing the probability that the two questions are duplicates.\n",
    "\n",
    "The model should take in each input sentence, replace it with it's embeddings, then run the new embedding vector through a LSTM layer. The output of each LSTM layer should be concatenated together and then a standard Dense model can be used.\n",
    "\n",
    "Make sure to note that you should only use one LSTM layer that is shared by both the left and the right half. \n",
    "\n",
    "Make sure to title your output layers as `predictions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5000, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 2,815,391\n",
      "Trainable params: 2,815,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lyric_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128,input_length = 5000))\n",
    "model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#predictions = model.predict(data_1_train.reshape(1,5000),batch_size=1,verbose = 2)[0]\n",
    "#model.add(LSTM(20000, return_sequences=True))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(10, activation='softmax'))\n",
    "#model.add(Activation('softmax'))\n",
    "\n",
    "#embedding\n",
    "#LSTM\n",
    "#hidden1\n",
    "#hidden2\n",
    "#hidden3\n",
    "#dense\n",
    "#output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(inputs=lyric_input, outputs=predictions)\n",
    "#model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 5000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Training the Model\n",
    "\n",
    "In this section we will simply train the model. We use the Early Stopping argument to end training if the loss or accuracy don't improve within 3 epochs.\n",
    "\n",
    "Since the training time is incredibly long (30 minutes or so on a CPU), only train it for one epoch if you don't have time. For better results, train it to around 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "192/192 [==============================] - 54s 280ms/step - loss: 1.0988 - acc: 0.3281\n",
      "Epoch 2/50\n",
      "192/192 [==============================] - 55s 288ms/step - loss: 1.0933 - acc: 0.3854\n",
      "Epoch 3/50\n",
      "192/192 [==============================] - 55s 284ms/step - loss: 1.0865 - acc: 0.5052\n",
      "Epoch 4/50\n",
      "192/192 [==============================] - 54s 284ms/step - loss: 1.0801 - acc: 0.4948\n",
      "Epoch 5/50\n",
      "192/192 [==============================] - 55s 288ms/step - loss: 1.0729 - acc: 0.4948\n",
      "Epoch 6/50\n",
      "192/192 [==============================] - 54s 284ms/step - loss: 1.0604 - acc: 0.4948\n",
      "Epoch 7/50\n",
      "192/192 [==============================] - 54s 284ms/step - loss: 1.0453 - acc: 0.4375\n",
      "Epoch 8/50\n",
      "192/192 [==============================] - 55s 286ms/step - loss: 1.0313 - acc: 0.4427\n",
      "Epoch 9/50\n",
      "192/192 [==============================] - 55s 288ms/step - loss: 1.0192 - acc: 0.4271\n",
      "Epoch 10/50\n",
      "192/192 [==============================] - 55s 286ms/step - loss: 0.9929 - acc: 0.4427\n",
      "Epoch 11/50\n",
      "192/192 [==============================] - 55s 286ms/step - loss: 0.9944 - acc: 0.4688\n",
      "Epoch 12/50\n",
      "192/192 [==============================] - 55s 288ms/step - loss: 0.9648 - acc: 0.5156\n",
      "Epoch 13/50\n",
      "192/192 [==============================] - 55s 286ms/step - loss: 0.9403 - acc: 0.5156\n",
      "Epoch 14/50\n",
      "192/192 [==============================] - 55s 286ms/step - loss: 0.9079 - acc: 0.5260\n",
      "Epoch 15/50\n",
      "192/192 [==============================] - 56s 289ms/step - loss: 0.9213 - acc: 0.4948\n",
      "Epoch 16/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.8510 - acc: 0.6458\n",
      "Epoch 17/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.8497 - acc: 0.7500\n",
      "Epoch 18/50\n",
      "192/192 [==============================] - 56s 290ms/step - loss: 0.8228 - acc: 0.8490\n",
      "Epoch 19/50\n",
      "192/192 [==============================] - 55s 286ms/step - loss: 0.7924 - acc: 0.8698\n",
      "Epoch 20/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.7496 - acc: 0.8906\n",
      "Epoch 21/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.6697 - acc: 0.9219\n",
      "Epoch 22/50\n",
      "192/192 [==============================] - 56s 289ms/step - loss: 0.6006 - acc: 0.8750\n",
      "Epoch 23/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.5572 - acc: 0.8542\n",
      "Epoch 24/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.5260 - acc: 0.7865\n",
      "Epoch 25/50\n",
      "192/192 [==============================] - 55s 288ms/step - loss: 0.4714 - acc: 0.8646\n",
      "Epoch 26/50\n",
      "192/192 [==============================] - 55s 284ms/step - loss: 0.3761 - acc: 0.9010\n",
      "Epoch 27/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.4388 - acc: 0.8177\n",
      "Epoch 28/50\n",
      "192/192 [==============================] - 55s 289ms/step - loss: 0.3809 - acc: 0.8854\n",
      "Epoch 29/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.2517 - acc: 0.9531\n",
      "Epoch 30/50\n",
      "192/192 [==============================] - 55s 284ms/step - loss: 0.2731 - acc: 0.9271\n",
      "Epoch 31/50\n",
      "192/192 [==============================] - 56s 289ms/step - loss: 0.2591 - acc: 0.9427\n",
      "Epoch 32/50\n",
      "192/192 [==============================] - 55s 286ms/step - loss: 0.2647 - acc: 0.9271\n",
      "Epoch 33/50\n",
      "192/192 [==============================] - 55s 284ms/step - loss: 0.2338 - acc: 0.9531\n",
      "Epoch 34/50\n",
      "192/192 [==============================] - 55s 284ms/step - loss: 0.1687 - acc: 0.9583\n",
      "Epoch 35/50\n",
      "192/192 [==============================] - 55s 288ms/step - loss: 0.1546 - acc: 0.9635\n",
      "Epoch 36/50\n",
      "192/192 [==============================] - 55s 287ms/step - loss: 0.1460 - acc: 0.9740\n",
      "Epoch 37/50\n",
      "192/192 [==============================] - 56s 290ms/step - loss: 0.1381 - acc: 0.9740\n",
      "Epoch 38/50\n",
      "192/192 [==============================] - 56s 294ms/step - loss: 0.1166 - acc: 0.9844\n",
      "Epoch 39/50\n",
      "192/192 [==============================] - 56s 291ms/step - loss: 0.0997 - acc: 0.9844\n",
      "Epoch 40/50\n",
      "192/192 [==============================] - 56s 291ms/step - loss: 0.0925 - acc: 0.9844\n",
      "Epoch 41/50\n",
      "192/192 [==============================] - 57s 295ms/step - loss: 0.0808 - acc: 0.9844\n",
      "Epoch 42/50\n",
      "192/192 [==============================] - 56s 290ms/step - loss: 0.0891 - acc: 0.9896\n",
      "Epoch 43/50\n",
      "192/192 [==============================] - 55s 284ms/step - loss: 0.0595 - acc: 0.9844\n",
      "Epoch 44/50\n",
      "192/192 [==============================] - 55s 289ms/step - loss: 0.0634 - acc: 0.9896\n",
      "Epoch 45/50\n",
      "192/192 [==============================] - 55s 284ms/step - loss: 0.0591 - acc: 0.9896\n",
      "Epoch 46/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.0620 - acc: 0.9792\n",
      "Epoch 47/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.1098 - acc: 0.9635\n",
      "Epoch 48/50\n",
      "192/192 [==============================] - 56s 289ms/step - loss: 0.0328 - acc: 0.9896\n",
      "Epoch 49/50\n",
      "192/192 [==============================] - 54s 284ms/step - loss: 0.0328 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "192/192 [==============================] - 55s 285ms/step - loss: 0.0529 - acc: 0.9844\n"
     ]
    }
   ],
   "source": [
    "#early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "hist = model.fit([data_1_train], labels_train, \\\n",
    "        epochs=50, batch_size=2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "15/15 [==============================] - 2s 138ms/step\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "testlyrics = [clean_text(x) for x in test_df['Lyrics']]\n",
    "testlabels = test_df['Genre']\n",
    "labels_test = []\n",
    "for label in testlabels:\n",
    "    if(label == 'Happy'):\n",
    "        labels_test.append([0, 1, 0])\n",
    "    elif(label == 'Sad'):\n",
    "        labels_test.append([1, 0, 0])\n",
    "    else:\n",
    "        labels_test.append([0, 0, 1])\n",
    "\n",
    "labels_test = np.array(labels_test)\n",
    "testlyrics = np.array(testlyrics)\n",
    "                     \n",
    "test_sequences_1 = tokenizer.texts_to_sequences(testlyrics)\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "#print('Loaded Testing Data')\n",
    "\n",
    "y_pred = model.predict(test_data_1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "y_test_class = np.argmax(labels_test,axis=1)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.80      0.57         5\n",
      "          1       0.75      0.60      0.67         5\n",
      "          2       0.50      0.20      0.29         5\n",
      "\n",
      "avg / total       0.56      0.53      0.51        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "cnf_matrix = confusion_matrix(y_test_class,y_pred_class)\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Sad', 'Happy', 'Angry'], title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
